{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "effa3437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import string\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "stemmer = nltk.stem.snowball.EnglishStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3573af24",
   "metadata": {},
   "outputs": [],
   "source": [
    "closed_class_stop_words = ['a','the','an','and','or','but','about','above','after','along','amid','among',\\\n",
    "                           'as','at','by','for','from','in','into','like','minus','near','of','off','on',\\\n",
    "                           'onto','out','over','past','per','plus','since','till','to','under','until','up',\\\n",
    "                           'via','vs','with','that','can','cannot','could','may','might','must',\\\n",
    "                           'need','ought','shall','should','will','would','have','had','has','having','be',\\\n",
    "                           'is','am','are','was','were','being','been','get','gets','got','gotten',\\\n",
    "                           'getting','seem','seeming','seems','seemed',\\\n",
    "                           'enough', 'both', 'all', 'your' 'those', 'this', 'these', \\\n",
    "                           'their', 'the', 'that', 'some', 'our', 'no', 'neither', 'my',\\\n",
    "                           'its', 'his' 'her', 'every', 'either', 'each', 'any', 'another',\\\n",
    "                           'an', 'a', 'just', 'mere', 'such', 'merely' 'right', 'no', 'not',\\\n",
    "                           'only', 'sheer', 'even', 'especially', 'namely', 'as', 'more',\\\n",
    "                           'most', 'less' 'least', 'so', 'enough', 'too', 'pretty', 'quite',\\\n",
    "                           'rather', 'somewhat', 'sufficiently' 'same', 'different', 'such',\\\n",
    "                           'when', 'why', 'where', 'how', 'what', 'who', 'whom', 'which',\\\n",
    "                           'whether', 'why', 'whose', 'if', 'anybody', 'anyone', 'anyplace', \\\n",
    "                           'anything', 'anytime' 'anywhere', 'everybody', 'everyday',\\\n",
    "                           'everyone', 'everyplace', 'everything' 'everywhere', 'whatever',\\\n",
    "                           'whenever', 'whereever', 'whichever', 'whoever', 'whomever' 'he',\\\n",
    "                           'him', 'his', 'her', 'she', 'it', 'they', 'them', 'its', 'their','theirs',\\\n",
    "                           'you','your','yours','me','my','mine','I','we','us','much','and/or'\n",
    "                           ]\n",
    "\n",
    "stop_words = set([*nltk.corpus.stopwords.words('english'),\n",
    "                *closed_class_stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c94ef05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>combined_text</th>\n",
       "      <th>annual_salary</th>\n",
       "      <th>salary_range</th>\n",
       "      <th>salary_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hearing Care Provider Overview\\n\\nHearingLife ...</td>\n",
       "      <td>63000.0</td>\n",
       "      <td>50k-100k</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cook descriptionTitle\\n\\n Looking for a great ...</td>\n",
       "      <td>46321.6</td>\n",
       "      <td>0-50k</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Principal Cloud Security Architect (Remote) Jo...</td>\n",
       "      <td>240895.0</td>\n",
       "      <td>150k+</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dishwasher descriptionTitle\\n\\n $2,000 Sign-on...</td>\n",
       "      <td>40144.0</td>\n",
       "      <td>0-50k</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Insights Analyst - Auto Industry Who We Are\\n\\...</td>\n",
       "      <td>61000.0</td>\n",
       "      <td>50k-100k</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       combined_text  annual_salary  \\\n",
       "0  Hearing Care Provider Overview\\n\\nHearingLife ...        63000.0   \n",
       "1  Cook descriptionTitle\\n\\n Looking for a great ...        46321.6   \n",
       "2  Principal Cloud Security Architect (Remote) Jo...       240895.0   \n",
       "3  Dishwasher descriptionTitle\\n\\n $2,000 Sign-on...        40144.0   \n",
       "4  Insights Analyst - Auto Industry Who We Are\\n\\...        61000.0   \n",
       "\n",
       "  salary_range  salary_bin  \n",
       "0     50k-100k           1  \n",
       "1        0-50k           0  \n",
       "2        150k+           3  \n",
       "3        0-50k           0  \n",
       "4     50k-100k           1  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#file_path = 'job_postings_salary.txt'  // Mansheel's file path\n",
    "file_path = 'job_postings_salary.csv'\n",
    "all_postings = pd.read_csv(file_path)\n",
    "\n",
    "all_postings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "72664c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  \n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  \n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  \n",
    "                           u\"\\U0001F700-\\U0001F77F\"  \n",
    "                           u\"\\U0001F780-\\U0001F7FF\"  \n",
    "                           u\"\\U0001F800-\\U0001F8FF\"  \n",
    "                           u\"\\U0001F900-\\U0001F9FF\"  \n",
    "                           u\"\\U0001FA00-\\U0001FA6F\"  \n",
    "                           u\"\\U0001FA70-\\U0001FAFF\"  \n",
    "                           u\"\\U00002702-\\U000027B0\"  \n",
    "                           u\"\\U000024C2-\\U0001F251\" \n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "454969c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text_no_emojis = remove_emojis(text)\n",
    "    tokens = word_tokenize(text_no_emojis)\n",
    "    processed_tokens = [stemmer.stem(token.lower()) for token in tokens \n",
    "                        if token.lower() not in stop_words and token not in string.punctuation]\n",
    "    return ' '.join(processed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ccc0bde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       combined_text  \\\n",
      "0  Hearing Care Provider Overview\\n\\nHearingLife ...   \n",
      "1  Cook descriptionTitle\\n\\n Looking for a great ...   \n",
      "2  Principal Cloud Security Architect (Remote) Jo...   \n",
      "3  Dishwasher descriptionTitle\\n\\n $2,000 Sign-on...   \n",
      "4  Insights Analyst - Auto Industry Who We Are\\n\\...   \n",
      "\n",
      "                               processed_description  \n",
      "0  hear care provid overview hearinglif nation he...  \n",
      "1  cook descriptiontitl look great opportun devel...  \n",
      "2  princip cloud secur architect remot job summar...  \n",
      "3  dishwash descriptiontitl 2,000 sign-on bonus g...  \n",
      "4  insight analyst auto industri escal award-win ...  \n"
     ]
    }
   ],
   "source": [
    "all_postings['processed_description'] = all_postings['combined_text'].apply(preprocess_text)\n",
    "\n",
    "print(all_postings[['combined_text', 'processed_description']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e5db5a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13350, 4)\n"
     ]
    }
   ],
   "source": [
    "print(all_postings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "04398161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define target class and input text\n",
    "target = all_postings['salary_bin'].astype('category')\n",
    "text = all_postings['combined_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "78063132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# split data into 70% training, 10% validation and 20% testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(text, target, test_size=0.2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.125)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9f44cffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf_vectorizer = tfidf.fit(X_train)\n",
    "X_train_tfidf = tfidf_vectorizer.transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81542d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the top20 words with the highest tf-idf score in each salary bin\n",
    "def print_top20_words(X, y, tfidf_vectorizer):\n",
    "    for i, salary_bin in enumerate(y.cat.categories):\n",
    "        X_salary_bin = X[y == salary_bin]\n",
    "        X_salary_bin_tfidf = tfidf_vectorizer.transform(X_salary_bin)\n",
    "        tfidf_scores = np.array(X_salary_bin_tfidf.mean(axis=0)).flatten()\n",
    "        top20_word_indices = np.argsort(tfidf_scores)[::-1][:20]\n",
    "        top20_words = np.array(tfidf_vectorizer.get_feature_names())[top20_word_indices]\n",
    "        print(f'Top 20 words in salary bin {salary_bin}: {top20_words}')\n",
    "\n",
    "print_top20_words(X_train, y_train, tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3d7fadce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# tune max depth, min samples split and min samples leaf, and n_estimators\n",
    "max_depth = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None]\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "n_estimators = [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_max_depth = None\n",
    "best_min_samples_split = None\n",
    "best_min_samples_leaf = None\n",
    "best_n_estimators = None\n",
    "\n",
    "for depth in max_depth:\n",
    "    for split in min_samples_split:\n",
    "        for leaf in min_samples_leaf:\n",
    "            for n in n_estimators:\n",
    "                rf = RandomForestClassifier(max_depth=depth, min_samples_split=split, min_samples_leaf=leaf, n_estimators=n)\n",
    "                rf.fit(X_train_tfidf, y_train)\n",
    "                y_val_pred = rf.predict(X_val_tfidf)\n",
    "                accuracy = accuracy_score(y_val, y_val_pred)\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    best_max_depth = depth\n",
    "                    best_min_samples_split = split\n",
    "                    best_min_samples_leaf = leaf\n",
    "                    best_n_estimators = n\n",
    "\n",
    "print(f'Best accuracy: {best_accuracy}')\n",
    "print(f'Best max depth: {best_max_depth}')\n",
    "print(f'Best min samples split: {best_min_samples_split}')\n",
    "print(f'Best min samples leaf: {best_min_samples_leaf}')\n",
    "print(f'Best n estimators: {best_n_estimators}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26230e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=0, max_depth=30)\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63d33dc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca06be41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
