{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "effa3437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import string\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "stemmer = nltk.stem.snowball.EnglishStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3573af24",
   "metadata": {},
   "outputs": [],
   "source": [
    "closed_class_stop_words = ['a','the','an','and','or','but','about','above','after','along','amid','among',\\\n",
    "                           'as','at','by','for','from','in','into','like','minus','near','of','off','on',\\\n",
    "                           'onto','out','over','past','per','plus','since','till','to','under','until','up',\\\n",
    "                           'via','vs','with','that','can','cannot','could','may','might','must',\\\n",
    "                           'need','ought','shall','should','will','would','have','had','has','having','be',\\\n",
    "                           'is','am','are','was','were','being','been','get','gets','got','gotten',\\\n",
    "                           'getting','seem','seeming','seems','seemed',\\\n",
    "                           'enough', 'both', 'all', 'your' 'those', 'this', 'these', \\\n",
    "                           'their', 'the', 'that', 'some', 'our', 'no', 'neither', 'my',\\\n",
    "                           'its', 'his' 'her', 'every', 'either', 'each', 'any', 'another',\\\n",
    "                           'an', 'a', 'just', 'mere', 'such', 'merely' 'right', 'no', 'not',\\\n",
    "                           'only', 'sheer', 'even', 'especially', 'namely', 'as', 'more',\\\n",
    "                           'most', 'less' 'least', 'so', 'enough', 'too', 'pretty', 'quite',\\\n",
    "                           'rather', 'somewhat', 'sufficiently' 'same', 'different', 'such',\\\n",
    "                           'when', 'why', 'where', 'how', 'what', 'who', 'whom', 'which',\\\n",
    "                           'whether', 'why', 'whose', 'if', 'anybody', 'anyone', 'anyplace', \\\n",
    "                           'anything', 'anytime' 'anywhere', 'everybody', 'everyday',\\\n",
    "                           'everyone', 'everyplace', 'everything' 'everywhere', 'whatever',\\\n",
    "                           'whenever', 'whereever', 'whichever', 'whoever', 'whomever' 'he',\\\n",
    "                           'him', 'his', 'her', 'she', 'it', 'they', 'them', 'its', 'their','theirs',\\\n",
    "                           'you','your','yours','me','my','mine','I','we','us','much','and/or'\n",
    "                           ]\n",
    "\n",
    "stop_words = set([*nltk.corpus.stopwords.words('english'),\n",
    "                *closed_class_stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c94ef05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>annual_salary</th>\n",
       "      <th>salary_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Overview\\n\\nHearingLife is a national hearing ...</td>\n",
       "      <td>63000.0</td>\n",
       "      <td>50k-100k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>descriptionTitle\\n\\n Looking for a great oppor...</td>\n",
       "      <td>46321.6</td>\n",
       "      <td>0-50k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Job Summary\\nAt iHerb, we are on a mission to ...</td>\n",
       "      <td>240895.0</td>\n",
       "      <td>200k-250k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>descriptionTitle\\n\\n $2,000 Sign-on Bonus Guar...</td>\n",
       "      <td>40144.0</td>\n",
       "      <td>0-50k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who We Are\\n\\nEscalent is an award-winning dat...</td>\n",
       "      <td>61000.0</td>\n",
       "      <td>50k-100k</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  annual_salary  \\\n",
       "0  Overview\\n\\nHearingLife is a national hearing ...        63000.0   \n",
       "1  descriptionTitle\\n\\n Looking for a great oppor...        46321.6   \n",
       "2  Job Summary\\nAt iHerb, we are on a mission to ...       240895.0   \n",
       "3  descriptionTitle\\n\\n $2,000 Sign-on Bonus Guar...        40144.0   \n",
       "4  Who We Are\\n\\nEscalent is an award-winning dat...        61000.0   \n",
       "\n",
       "  salary_range  \n",
       "0     50k-100k  \n",
       "1        0-50k  \n",
       "2    200k-250k  \n",
       "3        0-50k  \n",
       "4     50k-100k  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'job_postings_salary.txt'  \n",
    "all_postings = pd.read_csv(file_path)\n",
    "\n",
    "all_postings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "72664c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  \n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  \n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  \n",
    "                           u\"\\U0001F700-\\U0001F77F\"  \n",
    "                           u\"\\U0001F780-\\U0001F7FF\"  \n",
    "                           u\"\\U0001F800-\\U0001F8FF\"  \n",
    "                           u\"\\U0001F900-\\U0001F9FF\"  \n",
    "                           u\"\\U0001FA00-\\U0001FA6F\"  \n",
    "                           u\"\\U0001FA70-\\U0001FAFF\"  \n",
    "                           u\"\\U00002702-\\U000027B0\"  \n",
    "                           u\"\\U000024C2-\\U0001F251\" \n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "454969c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text_no_emojis = remove_emojis(text)\n",
    "    tokens = word_tokenize(text_no_emojis)\n",
    "    processed_tokens = [stemmer.stem(token.lower()) for token in tokens \n",
    "                        if token.lower() not in stop_words and token not in string.punctuation]\n",
    "    return ' '.join(processed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ccc0bde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         description  \\\n",
      "0  Overview\\n\\nHearingLife is a national hearing ...   \n",
      "1  descriptionTitle\\n\\n Looking for a great oppor...   \n",
      "2  Job Summary\\nAt iHerb, we are on a mission to ...   \n",
      "3  descriptionTitle\\n\\n $2,000 Sign-on Bonus Guar...   \n",
      "4  Who We Are\\n\\nEscalent is an award-winning dat...   \n",
      "\n",
      "                               processed_description  \n",
      "0  overview hearinglif nation hear care compani p...  \n",
      "1  descriptiontitl look great opportun develop pr...  \n",
      "2  job summari iherb mission make health well acc...  \n",
      "3  descriptiontitl 2,000 sign-on bonus guarante l...  \n",
      "4  escal award-win data analyt advisori firm help...  \n"
     ]
    }
   ],
   "source": [
    "all_postings['processed_description'] = all_postings['description'].apply(preprocess_text)\n",
    "\n",
    "print(all_postings[['description', 'processed_description']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b580416d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13350, 89302)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(all_postings['processed_description'])\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "080dc398",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    00       000  000000  00004258  00008592  0000estim  0001  000122  \\\n",
      "0  0.0  0.000000     0.0       0.0       0.0        0.0   0.0     0.0   \n",
      "1  0.0  0.000000     0.0       0.0       0.0        0.0   0.0     0.0   \n",
      "2  0.0  0.010181     0.0       0.0       0.0        0.0   0.0     0.0   \n",
      "3  0.0  0.085519     0.0       0.0       0.0        0.0   0.0     0.0   \n",
      "4  0.0  0.070960     0.0       0.0       0.0        0.0   0.0     0.0   \n",
      "\n",
      "   00017837  0003  ...  école  éducat   él  éste  éstos  única  škoda  ǣcode  \\\n",
      "0       0.0   0.0  ...    0.0     0.0  0.0   0.0    0.0    0.0    0.0    0.0   \n",
      "1       0.0   0.0  ...    0.0     0.0  0.0   0.0    0.0    0.0    0.0    0.0   \n",
      "2       0.0   0.0  ...    0.0     0.0  0.0   0.0    0.0    0.0    0.0    0.0   \n",
      "3       0.0   0.0  ...    0.0     0.0  0.0   0.0    0.0    0.0    0.0    0.0   \n",
      "4       0.0   0.0  ...    0.0     0.0  0.0   0.0    0.0    0.0    0.0    0.0   \n",
      "\n",
      "   ǣpresenc  ǣscan  \n",
      "0       0.0    0.0  \n",
      "1       0.0    0.0  \n",
      "2       0.0    0.0  \n",
      "3       0.0    0.0  \n",
      "4       0.0    0.0  \n",
      "\n",
      "[5 rows x 89302 columns]\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "print(tfidf_df.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee5aecb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca06be41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
